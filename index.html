
<!DOCTYPE html>
<html lang="en">
<head>
<br/>
<br/>
<title>Human Motion Transfer</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0" charset="utf-8">
<!-- jQuery -->
<script src="http://code.jquery.com/jquery.min.js"></script>
<!-- Bootstrap -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
<!-- Bootstrap -->
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<link rel="stylesheet" type="text/css" href="http://people.eecs.berkeley.edu/~shiry/styles/main.css"/>


<!-- ICONS -->
<link rel="icon" type="image/png" href="assets/icons/verlab_mini.png"/>
<link rel="icon" type="image/png" href="https://www.verlab.dcc.ufmg.br/wp-content/uploads/2013/09/SVG_Verlab_favicon900dpi-150x150.png/">
</head>
<body>

<div class="container">
<div class="row">
	<div class="col-md-12">
		<div class="page-header">
			<h1 class="text-center">A Shape-Aware Retargeting Approach to Transfer Human Motion and Appearance in Monocular Videos</h1>
		</div>
	</div>
</div>
<br>
<div class="row"> <!-- first names row-->
		<div class="col-md-3">
			<h5 class="text-center">Thiago L. Gomes</h5>
		</div>
		<div class="col-md-3">
			<h5 class="text-center"><a href="http://www-sop.inria.fr/members/Renato-Jose.Martins/index.html">Renato Martins</a></h5>
		</div>
		<div class="col-md-3">
			<h5 class="text-center"><a href="https://jpeumesmo.github.io/">João P. Ferreira</a></h5>
		</div>
		<div class="col-md-3">
			<h5 class="text-center">Rafael Azevedo</h5>
		</div>
</div>
<div class="row"> <!-- second names row-->
		<div class="col-md">
			<h5 class="text-center">Guilherme Torres</h5>
		</div>
		<div class="col-md">
			<h5 class="text-center"><a href="https://homepages.dcc.ufmg.br/~erickson/">Erickson R. Nascimento</a></h5>
		</div>
</div>
<br>
<div class="row">
	<div class="col-md-2">
	</div>
	<div class="col-md-5">
		<h5 class="text-right"><a href="https://www.verlab.dcc.ufmg.br/">Universidade Federal de Minas Gerais</a></h5>
	</div>
	<div class="col-md-2">
		<h5 class="text-center">INRIA</h5>
	</div>
	<div class="col-md-2">
	</div>
</div>

<br>
<div class="row">
	<div class="col-md-12">
		<h5 class="text-center">International Journal of Computer Vision (IJCV)</h5>
	</div>
<div>
<br>
<div class="row">
	<div class="col-md-3">
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="https://github.com/verlab/ShapeAwareHumanRetargeting_IJCV_2021">[Code]</a></h3>
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="https://github.com/verlab/ShapeAwareHumanRetargeting_IJCV_2021#dataset">[Data]</a></h3>
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="https://arxiv.org/abs/2103.15596">[Arxiv]</a></h3>
	</div>
<!-- </div>
<div class="row"> -->
	<div class="col-md-3">
		<h3 class="text-left"><a href="https://www.verlab.dcc.ufmg.br/retargeting-motion">[Project Page]</a></h3>
	</div>
	<div class="col-md-5">
	</div>
<div>

<div class="row">
	<div class="col-md-1">
		<!--left margin column-->
	</div>

	<div class="col-md-10 text-center"> <!--main content column-->

	<p>
	    <figure class="figure">
	      <img src="assets/teaser.png" class="img-fluid mx-auto" alt="XXX">
	      <figcaption class="figure-caption"><b>Overview of our retargeting approach.</b> Our method is composed of four main components: human motion estimation in the source video (first component); we retarget this motion into a different target character (second component), considering the motion constraints (third component), and by last, we synthesize the appearance of the target character into the source video.</figcaption>
	    </figure>
	</p>

	<br>
    <h2>Abstract</h2>
     <br>
    <p class="text-justify">
		Transferring human motion and appearance between videos of human actors remains one of the key challenges in Computer Vision. Despite the advances from recent image-to-image translation approaches, there are several transferring contexts where most end-to-end learning-based retargeting methods still perform poorly. Transferring human appearance from one actor to another is only ensured when a strict setup has been complied, which is generally built considering their training regime's specificities. In this work, we propose a shape-aware approach based on a hybrid image-based rendering technique that exhibits competitive visual retargeting quality compared to state-of-the-art neural rendering approaches. The formulation leverages the user body shape into the retargeting while considering physical constraints of the motion in 3D and the 2D image domain. We also present a new video retargeting benchmark dataset composed of different videos with annotated human motions to evaluate the task of synthesizing people's videos, which can be used as a common base to improve tracking the progress in the field. The dataset and its evaluation protocols are designed to evaluate retargeting methods in more general and challenging conditions. Our method is validated in several experiments, comprising publicly available videos of actors with different shapes, motion types, and camera setups.
    </p>

	<br>
 	<hr>
 	<br>

 <h2>Paper (Arxiv preprint)</h2>
  <br>
    <ul class="media-list, citations">
    <li class="media" id="gestures">
        <a class="pull-left" href="https://arxiv.org/abs/2103.15596">
            <img class="media-object img-fluid img-thumbnail mr-4" src="assets/paper.png" alt="gestures paper" width="150">
        </a>
        <div class="media-body">
            <h5 class="media-heading text-left">A Shape-Aware Retargeting Approach to Transfer Human Motion and Appearance in Monocular Videos</h5>
            <p class="text-left">
			  Thiago L. Gomes, Renato Martins, João P. Ferreira, Rafael Azevedo, Guilherme Torres and Erickson R. Nascimento. 
              <span class="cite-title">Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio</span>,
              Elsevier Computers and Graphics, C&A, 2020.
              <br>
              <a href="https://arxiv.org/abs/2103.15596">PDF</a>, <a href="#gestures" role="button" data-toggle="collapse" data-target="#collapse-gestures" aria-expanded="false" aria-controls="collapse">BibTeX</a>
              <div class="collapse" id="collapse-gestures">
              <div class="card text-left bg-light mb-4">
                @article{ferreira2020cag,<br> 
        &nbsp;&nbsp;author={João P. Ferreira and Thiago M. Coutinho and Thiago L. Gomes and José F. Neto and Rafael Azevedo and Renato Martins and Erickson R. Nascimento},<br> 
        &nbsp;&nbsp;title = {Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio},<br>
        &nbsp;&nbsp;journal = {Computers & Graphics}<br> 
        &nbsp;&nbsp;volume = {94},<br>
        &nbsp;&nbsp;pages = {11 - 21},<br>
        &nbsp;&nbsp;year={2021},<br> 
		&nbsp;&nbsp;issn={0097-8493}<br>
		&nbsp;&nbsp;doi={https://doi.org/10.1016/j.cag.2020.09.009}<br>
        }
              </div>
              </div>
            </p>
        </div>
    </li>
  </ul>

    <br>
 	<hr>
 	<br>

 	<h2>Demo</h2>
 	<br>
	 <iframe width="560" height="315" src="https://www.youtube.com/embed/fGDK6UkKzvA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  	<br>
 	<hr>
 	<br>

	<h2>Code</h2>
	<p>
	<h3 class="text-center"><a href="https://github.com/verlab/ShapeAwareHumanRetargeting_IJCV_2021">[PyTorch]</a></h3>
	</p>

 	<br>
 	<hr>
 	<br>

   	 <h2>Data</h2>
   	 <br>
	<p>
		<figure class="figure">
			<img src="assets/dataset.png" class="img-fluid mx-auto" alt="XXX">
			<figcaption class="figure-caption"><b>Human retargeting dataset.</b> a) The subjects participating in our dataset, their respective height and estimated SMPL body models. b) Overview of all motions available in our proposed dataset. c) Paired motions (upper and lower rows) with annotated motion constraints (3D constraints in blue and 2D constraints in red). d) The reconstructed 3D motions.</figcaption>
		</figure>
	</p>
   	 <!-- <p class="text-justify">
   	 	Note: the data for Conan was updated recently to remove duplicate videos. The numerical results pertaining to Conan will be updated soon. 
   	 </p> -->
   	<div>
   		<a href="https://github.com/verlab/ShapeAwareHumanRetargeting_IJCV_2021#dataset" class="btn btn-outline-secondary">Download</a>
   	</div>


	<br>
	<hr>
	<br>
  
	<h2>Concurrent work</h2>
	   <br>
	   <p class="text-justify">
			Concurrently and independently from us, a number of groups have proposed closely related — and very interesting! — methods for dance generation from music. Here is a partial list:
		</p>
		<ul class="text-left">
			<li>
				Xuanchi Ren, Haoran Li, Zijian Huang, Qifeng Chen. <i><a href="https://arxiv.org/pdf/1912.06606.pdf">Music-oriented Dance Video Synthesis with Pose Perceptual Loss
				</a></i>
			</li>
			<li>
				Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, Daxin Jiang. <a href="https://arxiv.org/pdf/2006.06119.pdf"><i>Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning</i></a>
			</li>
			<li>
				Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, Hao Li. <i><a href="https://arxiv.org/pdf/2008.08171.pdf">Learning to Generate Diverse Dance Motions with Transformer
				</a></i>
			</li>
			<li>
				Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo Meng, Yanfeng Wang. <i><a href="https://arxiv.org/pdf/2009.07637.pdf">ChoreoNet: Towards Music to Dance Synthesis with Choreographic Action Unit
				</a></i>
			</li>
			<li>
				Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, Ming-Hsuan Yang, Jan Kautz. <i><a href="https://arxiv.org/pdf/1911.02001.pdf">Dancing to Music</a></i>
			</li>
		</ul>



   	<br>
 	<hr>
 	<br>

 	 <h2>Acknowledgements</h2>
 	  <br>
 	 <p class="text-justify">
		 The authors thank CAPES, CNPq, and FAPEMIG for funding this work. We also thank NVIDIA for the donation of a Titan XP GPU used in this research. We use as template for this webpage the page of <a href="http://people.eecs.berkeley.edu/~shiry/projects/speech2gesture/">Speech2Gesture.</a>
 	 </p>

	</div> <!-- close middle column -->
	<div class="col-md-1">
	<!-- empty room saver on right -->
	</div>
</div> <!-- close main row -->
</div> <!-- close body container --> 
<footer>
  <br/>
  <br/>
</footer>
</div>
</body>
</html>

