
<!DOCTYPE html>
<html lang="en">
<head>
<br/>
<br/>
<title>Learning to Dance</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0" charset="utf-8">
<!-- jQuery -->
<script src="http://code.jquery.com/jquery.min.js"></script>
<!-- Bootstrap -->
<link rel="stylesheet" href="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
<!-- Bootstrap -->
<script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.1.3/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
<link rel="stylesheet" type="text/css" href="http://people.eecs.berkeley.edu/~shiry/styles/main.css"/>


<!-- ICONS -->
<link rel="icon" type="image/png" href="assets/icons/verlab_mini.png"/>
<link rel="icon" type="image/png" href="https://www.verlab.dcc.ufmg.br/wp-content/uploads/2013/09/SVG_Verlab_favicon900dpi-150x150.png/">
</head>
<body>

<div class="container">
<div class="row">
	<div class="col-md-12">
		<div class="page-header">
			<h1 class="text-center">Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio</h1>
		</div>
	</div>
</div>
<br>
<div class="row"> <!-- first names row-->
		<div class="col-md-2">
			<h5 class="text-center"><a href="https://jpeumesmo.github.io/">João P. Ferreira</a></h5>
		</div>
		<div class="col-md-3">
			<h5 class="text-center"><a href="https://thiagomcoutinho.github.io/">Thiago M. Coutinho</a></h5>
		</div>
		<div class="col-md-3">
			<h5 class="text-center">Thiago L. Gomes</h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center"><a href="http://netolcc06.github.io/">José F. Neto</a></h5>
		</div>
		<div class="col-md-2">
			<h5 class="text-center">Rafael Azevedo</h5>
		</div>
</div>
<div class="row"> <!-- second names row-->
		<div class="col-md">
			<h5 class="text-center"><a href="http://www-sop.inria.fr/members/Renato-Jose.Martins/index.html">Renato Martins</a></h5>
		</div>
		<div class="col-md">
			<h5 class="text-center"><a href="https://homepages.dcc.ufmg.br/~erickson/">Erickson R. Nascimento</a></h5>
		</div>
</div>
<br>
<div class="row">
	<div class="col-md-2">
	</div>
	<div class="col-md-5">
		<h5 class="text-right"><a href="https://www.verlab.dcc.ufmg.br/">Universidade Federal de Minas Gerais</a></h5>
	</div>
	<div class="col-md-2">
		<h5 class="text-center">INRIA</h5>
	</div>
	<div class="col-md-2">
	</div>
</div>

<br>
<div class="row">
	<div class="col-md-12">
		<h5 class="text-center">Computers & Graphics</h5>
	</div>
<div>
<br>
<div class="row">
	<div class="col-md-4">
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="https://github.com/verlab/Learning2Dance_CAG_2020">[Code]</a></h3>
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="https://github.com/verlab/Learning2Dance_CAG_2020/blob/master/dataset.md">[Data]</a></h3>
	</div>
	<div class="col-md-1">
		<h3 class="text-center"><a href="https://arxiv.org/abs/2011.12999">[Arxiv]</a></h3>
	</div>
<!-- </div>
<div class="row"> -->
	<div class="col-md-3">
		<h3 class="text-left"><a href="https://www.verlab.dcc.ufmg.br/motion-analysis/cag2020/">[Project Page]</a></h3>
	</div>
	<div class="col-md-5">
	</div>
<div>

<div class="row">
	<div class="col-md-1">
		<!--left margin column-->
	</div>

	<div class="col-md-10 text-center"> <!--main content column-->

	<p>
	    <figure class="figure">
	      <img src="assets/learning_to_dance.png" class="img-fluid mx-auto" alt="XXX">
	      <figcaption class="figure-caption">Learning to Dance, a framework to generate human motion from an audio input using Graph Convolutional Networks.</figcaption>
	    </figure>
	</p>

	<br>
    <h2>Abstract</h2>
     <br>
    <p class="text-justify">
		Synthesizing human motion through learning techniques is becoming an increasingly popular approach to alleviating the requirement of new data capture to produce animations. Learning to move naturally from audio, and in particular to dance, is one of the more complex motions humans often perform effortlessly. Each dance movement is unique, yet such movements maintain the core characteristics of the dance style. Most approaches addressing this problem with classical convolutional and recursive neural models undergo training and variability issues due to the non-Euclidean geometry of the motion manifold structure.In this paper, we design a novel method based on graph convolutional networks to tackle the problem of automatic dance generation from audio information. Our method is capable of generating natural motions, preserving the music style key moves across different generated motion samples, by taking advantage of an adversarial learning scheme conditioned on the input music audios. We evaluate our method using a user study and with several quantitative metrics of the generated motions' distributions. The results suggest the proposed GCN model outperforms the current state of the art dance generation method conditioned on music in different experiments. Moreover, our graph-convolutional approach is simpler, easier to be trained, and generates more realistic motion styles regarding qualitative and different quantitative metrics than state of the art, and with a visual movement perceptual quality even comparable to real motion data.
    </p>

	<br>
 	<hr>
 	<br>

 <h2>Paper</h2>
  <br>
    <ul class="media-list, citations">
    <li class="media" id="gestures">
        <a class="pull-left" href="https://arxiv.org/abs/2011.12999">
            <img class="media-object img-fluid img-thumbnail mr-4" src="assets/paper.png" alt="gestures paper" width="150">
        </a>
        <div class="media-body">
            <h5 class="media-heading text-left">Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio</h5>
            <p class="text-left">
			  João P. Ferreira, Thiago M. Coutinho, Thiago L. Gomes, Jośe F. Neto, Rafael Azevedo, Renato Martins and Erickson R. Nascimento. 
              <span class="cite-title">Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio</span>,
              Elsevier Computers and Graphics, C&A, 2020.
              <br>
              <a href="https://www.sciencedirect.com/science/article/pii/S0097849320301436?via%3Dihub">PDF</a>, <a href="#gestures" role="button" data-toggle="collapse" data-target="#collapse-gestures" aria-expanded="false" aria-controls="collapse">BibTeX</a>
              <div class="collapse" id="collapse-gestures">
              <div class="card text-left bg-light mb-4">
                @article{ferreira2020cag,<br> 
        &nbsp;&nbsp;author={João P. Ferreira and Thiago M. Coutinho and Thiago L. Gomes and José F. Neto and Rafael Azevedo and Renato Martins and Erickson R. Nascimento},<br> 
        &nbsp;&nbsp;title = {Learning to dance: A graph convolutional adversarial network to generate realistic dance motions from audio},<br>
        &nbsp;&nbsp;journal = {Computers & Graphics}<br> 
        &nbsp;&nbsp;volume = {94},<br>
        &nbsp;&nbsp;pages = {11 - 21},<br>
        &nbsp;&nbsp;year={2021},<br> 
		&nbsp;&nbsp;issn={0097-8493}<br>
		&nbsp;&nbsp;doi={https://doi.org/10.1016/j.cag.2020.09.009}<br>
        }
              </div>
              </div>
            </p>
        </div>
    </li>
  </ul>

    <br>
 	<hr>
 	<br>

 	<h2>Demo</h2>
 	<br>
	 <iframe width="560" height="315" src="https://www.youtube.com/embed/fGDK6UkKzvA" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
  	<br>
 	<hr>
 	<br>

	<h2>Code</h2>
	<p>
	<h3 class="text-center"><a href="https://github.com/verlab/Learning2Dance_CAG_2020">[PyTorch]</a></h3>
	</p>

 	<br>
 	<hr>
 	<br>

   	 <h2>Data</h2>
   	 <br>
   	 <div>
   	    <img src="assets/dataset.png" alt="Dataset" width=auto>
 	</div>
	<br>
   	 <p class="text-justify">
		We build a new dataset composed of paired videos of people dancing different music styles. The dataset is used to train and evaluate the methodologies for motion generation from audio. We split the samples into training and evaluation sets that contain multimodal data for three music/dance styles: Ballet, Michael Jackson, and Salsa.  These two sets are composed of two data types: visual data from careful-selected parts of publicly available videos of dancers performing representative movements of the music style and audio data from the styles we are training. The Figure above shows some data samples of our dataset.

   	 </p>
   	 <!-- <p class="text-justify">
   	 	Note: the data for Conan was updated recently to remove duplicate videos. The numerical results pertaining to Conan will be updated soon. 
   	 </p> -->
   	<div>
   		<a href="https://github.com/verlab/Learning2Dance_CAG_2020/blob/master/dataset.md" class="btn btn-outline-secondary">Download</a>
   	</div>


	<br>
	<hr>
	<br>
  
	<h2>Concurrent work</h2>
	   <br>
	   <p class="text-justify">
			Concurrently and independently from us, a number of groups have proposed closely related — and very interesting! — methods for dance generation from music. Here is a partial list:
		</p>
		<ul class="text-left">
			<li>
				Xuanchi Ren, Haoran Li, Zijian Huang, Qifeng Chen. <i><a href="https://arxiv.org/pdf/1912.06606.pdf">Music-oriented Dance Video Synthesis with Pose Perceptual Loss
				</a></i>
			</li>
			<li>
				Ruozi Huang, Huang Hu, Wei Wu, Kei Sawada, Mi Zhang, Daxin Jiang. <a href="https://arxiv.org/pdf/2006.06119.pdf"><i>Dance Revolution: Long-Term Dance Generation with Music via Curriculum Learning</i></a>
			</li>
			<li>
				Jiaman Li, Yihang Yin, Hang Chu, Yi Zhou, Tingwu Wang, Sanja Fidler, Hao Li. <i><a href="https://arxiv.org/pdf/2008.08171.pdf">Learning to Generate Diverse Dance Motions with Transformer
				</a></i>
			</li>
			<li>
				Zijie Ye, Haozhe Wu, Jia Jia, Yaohua Bu, Wei Chen, Fanbo Meng, Yanfeng Wang. <i><a href="https://arxiv.org/pdf/2009.07637.pdf">ChoreoNet: Towards Music to Dance Synthesis with Choreographic Action Unit
				</a></i>
			</li>
			<li>
				Hsin-Ying Lee, Xiaodong Yang, Ming-Yu Liu, Ting-Chun Wang, Yu-Ding Lu, Ming-Hsuan Yang, Jan Kautz. <i><a href="https://arxiv.org/pdf/1911.02001.pdf">Dancing to Music</a></i>
			</li>
		</ul>



   	<br>
 	<hr>
 	<br>

 	 <h2>Acknowledgements</h2>
 	  <br>
 	 <p class="text-justify">
		  This work was supported, in part, by Nvidia, Cnpq, Capes and Fapemig. We use as template for this webpage the page of <a href="http://people.eecs.berkeley.edu/~shiry/projects/speech2gesture/">Speech2Gesture</a>
 	 </p>

	</div> <!-- close middle column -->
	<div class="col-md-1">
	<!-- empty room saver on right -->
	</div>
</div> <!-- close main row -->
</div> <!-- close body container --> 
<footer>
  <br/>
  <br/>
</footer>
</div>
</body>
</html>

